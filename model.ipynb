{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN Computer vision with Fashion MNIST from Zalando\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Dataset\n",
    "\n",
    "The main difference between loading image dataset with keras\n",
    "instead of scikitLearn is that in keras, each of them is represented\n",
    "as a 28x28 array rather than an array of size 784.\n",
    "+\n",
    "In keras, pixel intensities are represented as intergers from 0 to 255\n",
    "instead of floats from 0.0 to 255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fashion MNIST dataset\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load train and test data ( Total of 70_000 samples )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train and test data\n",
    "(x_fashion_train, y_fashion_train), (x_fashion_test, y_fashion_test) = fashion_mnist.load_data()\n",
    "\n",
    "x_train, y_train = x_fashion_train / 255.0, y_fashion_train\n",
    "x_test = x_fashion_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fashion MNIST Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fashion MNIST Data Labels glossaire\n",
    "# The first column is the label\n",
    "class_names = [\n",
    "    \"T-shirt/top\", # 0 \n",
    "    \"Trouser\",     # 1\n",
    "    \"Pullover\",    # 2\n",
    "    \"Dress\",       # 3\n",
    "    \"Coat\",        # 4\n",
    "    \"Sandal\",      # 5\n",
    "    \"Shirt\",       # 6\n",
    "    \"Sneaker\",     # 7\n",
    "    \"Bag\",         # 8\n",
    "    \"Ankle boot\"   # 9\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building our NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "\n",
    "    # For preprocessing\n",
    "    layers.Flatten(input_shape=[28,28]), # Transform 28x28 image data into 1D array because default is 2D\n",
    "\n",
    "    # Creating hidden layer - Manages its own weight matrix with connections weight between neurons. \n",
    "    # It manages a Bias vector too * once per neuron\n",
    "\n",
    "    # ReLU to make sure that everything is greater than 0 ( No negative numbers )\n",
    "    \n",
    "    layers.Dense(300, activation='relu'), # Create a Hidden Layer of 300 neurons\n",
    "    layers.Dense(100, activation='relu'), # Create a Hidden Layer of 100 neurons\n",
    "\n",
    "    # The Output Layer - Last Layer - 10 because there's 10 classnames ( So it will give us 10 probabilities - index from 0 to 9)\n",
    "    layers.Dense(10, activation='softmax') # Normalize the Output with SoftMax\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compiling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=\"sgd\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    1,          # Batch Size / 1 Batch size = all the samples, Default is 32 (1719 samples per batch)\n",
    "    epochs=30,  # Number of time it will go through all the samples\n",
    "    validation_split=0.1 # Take the last 10% of samples and use them to validate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing our model by predicting test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_toPredict = x_test[:1] # Take the first test sample and predict it\n",
    "y_proba = model.predict(x_toPredict) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the model with test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result = model.evaluate(\n",
    "    x_test, \n",
    "    y_fashion_test,\n",
    "    batch_size=1\n",
    ")\n",
    "# format is [0.loss, 0.accuracy]\n",
    "\n",
    "# print([round(e,2) for e in eval_result])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_proba.round(2))\n",
    "\n",
    "plt.imshow(x_test[0])\n",
    "plt.title(class_names[y_fashion_test[0]])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
